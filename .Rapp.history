load("/Users/peteralstone/Dropbox/_ERG-courses/2012-fall/ERG 254 -- AC Power/CAISO-data/myworkFridayAMAM.RData")
require(ggplot2)  # for nice plots#
require(plyr)     # for data processing#
require(reshape)  # for melting and casting data #
                  #(and you actually have to use reshape not reshape2...melting issues...)#
# require(RNCEP)    # for getting global weather and climate data on 2.5 degree grid.#
# require(grid)#
#
# INPUTS#
#
# start <- 20110101 #not used#
# end <- 20120101 #not used#
#
# setwd....#
# imac#
# rootDir <- "~/Desktop/Dropbox/_ERG-courses/2012-fall/ERG 254 -- AC Power/CAISO-data/"  #set root directory #
#mbp#
rootDir <- "~/Dropbox/_ERG-courses/2012-fall/ERG 254 -- AC Power/CAISO-data/"  #set root directory #
#
newData <- FALSE  # Fetch new data online?#
rootData <- "v0" #set location of data to use (if not fetched, the name of the dir...if fetched, the new name)#
#
# SYS PREP------------------------------------------------#
#
setwd(rootDir)#
#
# FUNCTIONS ------------------------------------------------#
#
# fetch CAISO LMP data from OASIS (see "Interface Specification for OASIS API)#
getCAISOlmp <- function(startdate=20110101, enddate=20110102, market_run_id="DAM",node="ALL",#
                        onlyDo=NA,allAtOnce=TRUE){#
#
# # DON"T use node by node , only use ALL nodes -- doesn't append data!#
#  this function simply grabs the csv data from caiso and dumps it into a data frame with the same structure as the csv.#
# onlyDo means "only do the first X nodes in the list"#
# allAtOnce means to use the "all apnode" command to download instead of node by node crawling -- use anything else at your peril.#
  # convert CAISO format to POSIXct dates#
  start <- strptime(startdate,"%Y%m%d")#
  end <- strptime(enddate,"%Y%m%d")#
  # Initialize data frame with starting day#
  activeDay <- start#
  #define base URL#
  baseURL <- "http://oasis.caiso.com/mrtu-oasis/SingleZip?"#
  if(node!="ALL"){ #if there is only one node...just do that.#
    while(activeDay < end){#
        activeNode <- node  #
        # assemble url for LMP#
        getURL <- paste(baseURL,"resultformat=6&queryname=PRC_LMP&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",market_run_id,"&node=",activeNode,sep="")#
        temp <- tempfile() #create temp file#
        download.file(getURL,temp) # get data into temp#
        data <- read.csv(unzip(temp)) # unzip and read csv, dump into data#
        unlink(temp)#
      #end for#
      activeDay <- activeDay + 86400 # add one day (in seconds)#
      #end while#
    }#
    return(data)#
  }else{ #...or get all of the nodes...#
if(allAtOnce){ #get all nodes day by day.  #
  # First day -- initialize data output frame.#
    getURL <- paste(baseURL,"resultformat=6&queryname=PRC_LMP&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",market_run_id,"&grp_type=ALL_APNODES",sep="")#
    temp <- tempfile() #create temp file#
    try(download.file(getURL,temp)) # get data into temp#
    try(data <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
    unlink(temp)#
    activeDay <- activeDay + 86400 #increment one day#
    while(activeDay < end){#
        # assemble url for LMP#
        getURL <- paste(baseURL,"resultformat=6&queryname=PRC_LMP&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",market_run_id,"&grp_type=ALL_APNODES",sep="")#
        temp <- tempfile() #create temp file#
        try(download.file(getURL,temp)) # get data into temp#
        try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
        unlink(temp)#
        try(data <- rbind(data,newdata))#append new data to existing#
        activeDay <- activeDay + 86400 # add one day (in seconds)#
    }    #
}else{    # go through pnodes one by one (very slow)#
  #get list of all Pnodes (this may or may not be all...just take the start date)#
  pnodeURL <- paste(baseURL,"resultformat=6&queryname=ATL_PNODE&Pnode_type=ALL&startdate=",strftime(start,"%Y%m%d"),sep="")#
  temp <- tempfile() #create temp file#
  download.file(pnodeURL,temp) # get data into temp#
  pnode.desc <- read.csv(unzip(temp)) # unzip and read csv, dump into data#
  unlink(temp)#
  if(is.na(onlyDo)){numberNodes <- length(pnode.desc$PNODE_ID)}else{numberNodes <- onlyDo}#
  activeNode <- pnode.desc$PNODE_ID[1]#
  # assemble url for LMP and download.#
  getURL <- paste(baseURL,"resultformat=6&queryname=PRC_LMP&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",market_run_id,"&node=",activeNode,sep="")#
  temp <- tempfile() #create temp file#
  download.file(getURL,temp) # get data into temp#
  data <- read.csv(unzip(temp)) # unzip and read csv, dump into data (THE MAIN OUTPUT)#
  unlink(temp)#
while(activeDay < end){#
  for(i in 2:numberNodes){#
  activeNode <- pnode.desc$PNODE_ID[i]  #
  # assemble url for LMP#
  getURL <- paste(baseURL,"resultformat=6&queryname=PRC_LMP&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",market_run_id,"&node=",activeNode,sep="")#
  temp <- tempfile() #create temp file#
  try(download.file(getURL,temp)) # get data into temp#
  try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
  unlink(temp)#
  try(data <- rbind(data,newdata))#append new data to existing DF#
  }#
  #end for#
  activeDay <- activeDay + 86400 # add one day (in seconds)#
#
} #end while#
} #end else#
  return(data)#
} #end else#
} #end FUNCTION ... getCAISOlmp#
#
# fetch CAISO energy clearing data from OASIS (see "Interface Specification for OASIS API)#
getCAISOsysenergy <- function(startdate=20110101, enddate=20110102, #
                              market_run_id=c("DAM","RTM","RUC","HASP")){#
  # this function simply grabs the csv data from caiso and dumps it into a data frame with the same structure as the csv.#
  # use a set of market run IDs to get all of them using c(oncatenate)...otherwise specify "DAM" "RTM" "RUC" or "HASP"#
  # convert CAISO format to POSIXct dates#
  start <- strptime(startdate,"%Y%m%d")#
  end <- strptime(enddate,"%Y%m%d")#
  # Initialize data frame with starting day#
  activeDay <- start#
  #define base URL#
  baseURL <- "http://oasis.caiso.com/mrtu-oasis/SingleZip?"#
      # First day -- initialize data output frame with first data.#
      dummy <- 1#
      for(i in market_run_id){#
      getURL <- paste(baseURL,"resultformat=6&queryname=ENE_SLRS&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",i,"&tac_zone_name=ALL&schedule=ALL",sep="")#
      temp <- tempfile() #create temp file#
      try(download.file(getURL,temp)) # get data into temp#
      if(dummy==1){try(data <- read.csv(unzip(temp)))#
      dummy=dummy+1#
        }else{#
        try(newdata <- read.csv(unzip(temp)))#
        try(data <- rbind(data,newdata))#append new data to existing#
      } # unzip and read csv, dump into data#
      unlink(temp)#
      } #end for loop for first day#
      activeDay <- activeDay + 86400 # add one day (in seconds)#
#
      # Subsequent days -- download and append#
      while(activeDay < end){      #
        for(i in market_run_id){ #loop through market types#
        getURL <- paste(baseURL,"resultformat=6&queryname=ENE_SLRS&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),"&market_run_id=",i,"&tac_zone_name=ALL&schedule=ALL",sep="")#
        temp <- tempfile() #create temp file#
        try(download.file(getURL,temp)) # get data into temp#
        try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
        unlink(temp)#
        try(data <- rbind(data,newdata))#append new data to existing#
        }#
        activeDay <- activeDay + 86400 # add one day (in seconds)#
      } #end while loop for moving through time...    #
    return(data) # these are the data you are looking for...#
} #end FUNCTION ... getCAISOsysenergy#
#
# fetch CAISO load data from OASIS (see "Interface Specification for OASIS API)#
getCAISOload <- function(startdate=20110101, enddate=20110102){#
  # this function simply grabs the csv data from caiso and dumps it into a data frame with the same structure as the csv.#
  # convert CAISO format to POSIXct dates#
  start <- strptime(startdate,"%Y%m%d")#
  end <- strptime(enddate,"%Y%m%d")#
  # Initialize data frame with starting day#
  activeDay <- start#
  #define base URL#
  baseURL <- "http://oasis.caiso.com/mrtu-oasis/SingleZip?"#
  # First day -- initialize data output frame with first data.#
    getURL <- paste(baseURL,"resultformat=6&queryname=SLD_FCST&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),sep="")#
    temp <- tempfile() #create temp file#
    try(download.file(getURL,temp)) # get data into temp#
    try(data <- read.csv(unzip(temp)))#
    unlink(temp)#
  activeDay <- activeDay + 86400 # add one day (in seconds)#
  # Subsequent days -- download and append#
  while(activeDay < end){      #
      getURL <- paste(baseURL,"resultformat=6&queryname=SLD_FCST&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),sep="")#
      temp <- tempfile() #create temp file#
      try(download.file(getURL,temp)) # get data into temp#
      try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
      unlink(temp)#
      try(data <- rbind(data,newdata))#append new data to existing#
    activeDay <- activeDay + 86400 # add one day (in seconds)#
  } #end while loop for moving through time...    #
  return(data) # these are the data you are looking for...#
} #end FUNCTION ... getCAISOload#
#
# Fetch CAISO public bid data (be careful, these are big files)#
getCAISObids <- function(startdate=20110101, enddate=20110102, market="DAM"){#
  #market is DAM or RTM#
  # this function simply grabs the csv data from caiso and dumps it into a data frame with the same structure as the csv.#
  # convert CAISO format to POSIXct dates#
  start <- strptime(startdate,"%Y%m%d")#
  end <- strptime(enddate,"%Y%m%d")#
  # Initialize data frame with starting day#
  activeDay <- start#
  #define base URL#
  baseURL <- "http://oasis.caiso.com/mrtu-oasis/GroupZip?" #note this is a grouped file#
  reportname<-paste("PUB_",market,"_GRP",sep="")#
  # First day -- initialize data output frame with first data.#
  getURL <- paste(baseURL,"resultformat=6&groupid=",reportname,"&startdate=",strftime(activeDay,"%Y%m%d"),sep="")#
  temp <- tempfile() #create temp file#
  try(download.file(getURL,temp)) # get data into temp#
  try(data <- read.csv(unzip(temp)))#
  unlink(temp)#
  activeDay <- activeDay + 86400 # add one day (in seconds)#
  # Subsequent days -- download and append#
  while(activeDay < end){      #
    getURL <- paste(baseURL,"resultformat=6&groupid=",reportname,"&startdate=",strftime(activeDay,"%Y%m%d"),sep="")#
    temp <- tempfile() #create temp file#
    try(download.file(getURL,temp)) # get data into temp#
    try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
    unlink(temp)#
    try(data <- rbind(data,newdata))#append new data to existing#
    activeDay <- activeDay + 86400 # add one day (in seconds)#
  } #end while loop for moving through time...    #
  return(data) # these are the data you are looking for...#
} #end FUNCTION ... getCAISObids#
# Fetch CAISO public bid data (be careful, these are big files)#
getCAISOopres <- function(startdate=20110101, enddate=20110102){#
  #market is DAM or RTM#
  # this function simply grabs the csv data from caiso and dumps it into a data frame with the same structure as the csv.#
  # convert CAISO format to POSIXct dates#
  start <- strptime(startdate,"%Y%m%d")#
  end <- strptime(enddate,"%Y%m%d")#
  # Initialize data frame with starting day#
  activeDay <- start#
  #define base URL#
  baseURL <- "http://oasis.caiso.com/mrtu-oasis/SingleZip?" #note this is a grouped file#
  # First day -- initialize data output frame with first data.#
  getURL <- paste(baseURL,"resultformat=6&queryname=AS_OP_RSRV&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),sep="")#
  temp <- tempfile() #create temp file#
  try(download.file(getURL,temp)) # get data into temp#
  try(data <- read.csv(unzip(temp)))#
  unlink(temp)#
  activeDay <- activeDay + 86400 # add one day (in seconds)#
  # Subsequent days -- download and append#
  while(activeDay < end){      #
    getURL <- paste(baseURL,"resultformat=6&queryname=AS_OP_RSRV&startdate=",strftime(activeDay,"%Y%m%d"),"&enddate=",strftime(activeDay,"%Y%m%d"),sep="")#
    temp <- tempfile() #create temp file#
    try(download.file(getURL,temp)) # get data into temp#
    try(newdata <- read.csv(unzip(temp))) # unzip and read csv, dump into data#
    unlink(temp)#
    try(data <- rbind(data,newdata))#append new data to existing#
    activeDay <- activeDay + 86400 # add one day (in seconds)#
  } #end while loop for moving through time...    #
  return(data) # these are the data you are looking for...#
} #end FUNCTION ... getCAISOopres#
#
# add date info to CAISO data frames#
addDatesCAISO <- function(caisodata,date.format=c("%Y%m%d"),date.column="OPR_DT"){#
caisodata$posixlt <- as.POSIXlt(caisodata[[date.column]])#
caisodata$year <- caisodata$posixlt$year+1900#
caisodata$mon <- strftime(caisodata$posixlt,"%b")#
caisodata$monnum <-strftime(caisodata$posixlt,"%m")#
caisodata$mday <- caisodata$posixlt$mday #
caisodata$wday <- strftime(caisodata$posixlt,"%a") #week day#
caisodata$date <- strftime(caisodata$posixlt,date.format)#
caisodata$yhr <- caisodata$posixlt$yday*24+caisodata$posixlt$hour+1 #hour ending 1-8760 over the year#
caisodata$yday <- caisodata$posixlt$yday#
#
return(caisodata)#
}#
#
# melt CAISO data into a mono-value data frame for analysis.#
meltCAISO <- function(caisodata,preserve.na=FALSE){#
hourLabels <- c("HE01","HE02","HE03","HE04","HE05","HE06","HE07","HE08","HE09","HE10","HE11","HE12","HE13","HE14","HE15","HE16","HE17","HE18","HE19","HE20","HE21","HE22","HE23","HE24")#
data <- melt(caisodata,id=which(!names(caisodata) %in% hourLabels),measured=which(names(caisodata) %in% hourLabels,na.rm=preserve.na))#
return(data)#
}
load("/Users/peteralstone/Dropbox/_SERC/LIGHTING/Lumina/RN_Embodied_energy/LCA-OGLP/finalAnalysis-upToPlots.RData")
load("/Users/peteralstone/Dropbox/_ERG/emac/building-data-exploration/wharton_pge_meters/spidobjects_alldates/allDates_ 547305.Rdata")
workspace()
ls
ls()
class(spidData)
names(spidData)
head(spidData)
dim(spidData)
pp <- spidData
class(pp$date)
levels(pp$date)
pp$date[1]
pp$date[1]->dd
as.POSIXlt(dd)
as.POSIXlt(dd)$y
as.POSIXlt(dd)$yday
pp$date<-as.POSIXlt(pp$date)
help(as.POSIXlt)
str(pp$date)
str(pp$date[1])
dd$date[1]$yday
pp$date[1]$yday
pp$date[1]$h
pp$date[1]$m
pp$date[1]$min
pp$date[1]$wday
pp$wday<-pp$date$wday
pp$mon<-pp$date$mon
head(pp)
pp$date[1]$hr
pp$date[1]$h
pp$hour <- pp$date$h
head(pp)
require(ggplot2)
myplot <- ggplot(pp, aes(x=hour, y=watts))
myplot+geom_jitter()+facet_grid(wday~mon)
hist(pp$watts)
good <- which(.is.na(pp$watts))
good <- which(is.na(pp$watts))
good <- which(!is.na(pp$watts))
myplot <- ggplot(pp[good], aes(x=hour, y=watts))
myplot <- ggplot(pp[good,], aes(x=hour, y=watts))
myplot+geom_points()
myplot+geom_point()
myplot+geom_point()+facet_grid(mon~wday)
myplot+geom_jitter()+facet_grid(mon~wday)+scale_y_log()
myplot+geom_jitter()+facet_grid(mon~wday)+scale_y_log10()
myplot+geom_jitter()+facet_grid(mon~wday)+scale_y_log10()+stat_smooth()
myplot+geom_jitter(size=0.5)+facet_grid(mon~wday)+scale_y_log10()+stat_smooth()
?fisher
?fisher.test
library(foreign) #this is to read in Stata data#
library(Hmisc)#
library(psych)#
library(stargazer)#
#
library(ggplot2) # for neato plotting tools#
library(plyr) # for nice data tools
ps1.data <- read.dta("ps1.dta") #changed name of object from "data" to avoid ambiguity issues since "data" is often embedded in functions as a general object#
#
print(nrow(ps1.data))#
## Problem 1a: Fix missing values#
## The following are the error codes for each of the 15 variables that need fixing:#
# cardiac: 9#
# lung: 9#
# diabetes: 9#
# herpes: 9#
# chyper: 9#
# phyper: 9#
# pre4000: 9#
# preterm: 9#
# tobacco: 9#
# cigar: 99#
# cigar6: 6#
# alcohol: 9#
# drink: 99#
# drink5: 5#
# wgain: 99#
#
# Identify which records have full data, then add a column to indicate full records or not.#
#
full.record.flag <- which(ps1.data$cardiac != 9 & ps1.data$lung != 9 & ps1.data$diabetes !=9 & ps1.data$herpes != 9 & ps1.data$chyper != 9 & ps1.data$phyper != 9 & ps1.data$pre4000 !=9 & ps1.data$preterm != 9 & ps1.data$tobacco != 9 & ps1.data$cigar != 99 & ps1.data$cigar6 !=6 & ps1.data$alcohol != 9 & ps1.data$drink != 99 & ps1.data$drink5 !=5 & ps1.data$wgain !=99)#
#
ps1.data$full.record <- FALSE # initialize column as F#
#
ps1.data$full.record[full.record.flag] <- TRUE #reassign level to T for full records#
#
# replace error rows in cigar with NA so they don't interfere with other calcs.#
error.cigar <- which(ps1.data$cigar == 99)#
ps1.data$cigar[error.cigar] <- NA#
#
# compare records on things that matter for this analysis...apgar, smoking, etc.#
ps1.compare.records <- ddply(ps1.data, .(full.record), summarize,#
                             mean.omaps = mean(omaps),#
                             sd.omaps = sd(omaps), #
                             mean.fmaps = mean(fmaps),#
                             sd.fmaps = sd(fmaps),#
                             median.cigar = median(cigar, na.rm = TRUE),#
                             per25.cigar = quantile(cigar,.25, na.rm = TRUE),#
                             per75.cigar = quantile(cigar,.75, na.rm = TRUE),#
                             per90.cigar = quantile(cigar,.90, na.rm = TRUE), #
                             mean.cigar = mean(cigar, na.rm = TRUE), #
                             sd.cigar = sd(cigar, na.rm = TRUE)#
                             )
ps1.data <- read.dta("ps1.dta") #changed name of object from "data" to avoid ambiguity issues since "data" is often embedded in functions as a general object
setwd(dir="Google Drive/ERG/Classes/ARE213/are213/")
ps1.data <- read.dta("ps1.dta") #changed name of object from "data" to avoid ambiguity issues since "data" is often embedded in functions as a general object#
#
print(nrow(ps1.data))#
## Problem 1a: Fix missing values#
## The following are the error codes for each of the 15 variables that need fixing:#
# cardiac: 9#
# lung: 9#
# diabetes: 9#
# herpes: 9#
# chyper: 9#
# phyper: 9#
# pre4000: 9#
# preterm: 9#
# tobacco: 9#
# cigar: 99#
# cigar6: 6#
# alcohol: 9#
# drink: 99#
# drink5: 5#
# wgain: 99#
#
# Identify which records have full data, then add a column to indicate full records or not.#
#
full.record.flag <- which(ps1.data$cardiac != 9 & ps1.data$lung != 9 & ps1.data$diabetes !=9 & ps1.data$herpes != 9 & ps1.data$chyper != 9 & ps1.data$phyper != 9 & ps1.data$pre4000 !=9 & ps1.data$preterm != 9 & ps1.data$tobacco != 9 & ps1.data$cigar != 99 & ps1.data$cigar6 !=6 & ps1.data$alcohol != 9 & ps1.data$drink != 99 & ps1.data$drink5 !=5 & ps1.data$wgain !=99)#
#
ps1.data$full.record <- FALSE # initialize column as F#
#
ps1.data$full.record[full.record.flag] <- TRUE #reassign level to T for full records#
#
# replace error rows in cigar with NA so they don't interfere with other calcs.#
error.cigar <- which(ps1.data$cigar == 99)#
ps1.data$cigar[error.cigar] <- NA#
#
# compare records on things that matter for this analysis...apgar, smoking, etc.#
ps1.compare.records <- ddply(ps1.data, .(full.record), summarize,#
                             mean.omaps = mean(omaps),#
                             sd.omaps = sd(omaps), #
                             mean.fmaps = mean(fmaps),#
                             sd.fmaps = sd(fmaps),#
                             median.cigar = median(cigar, na.rm = TRUE),#
                             per25.cigar = quantile(cigar,.25, na.rm = TRUE),#
                             per75.cigar = quantile(cigar,.75, na.rm = TRUE),#
                             per90.cigar = quantile(cigar,.90, na.rm = TRUE), #
                             mean.cigar = mean(cigar, na.rm = TRUE), #
                             sd.cigar = sd(cigar, na.rm = TRUE)#
                             )
ps1.data <- read.dta("ps1.dta") #changed name of object from "data" to avoid ambiguity issues since "data" is often embedded in functions as a general object
ls()
